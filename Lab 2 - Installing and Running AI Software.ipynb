{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/01-nvidia-logo-horiz-500x200-2c50-d@2x.png\" alt=\"NVIDIA Logo\" style=\"width: 300px; height: auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Installing and Running AI Software\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "### Audience\n",
    "The workbook is intended for technical training students.\n",
    "\n",
    "### Objectives\n",
    "In this practice, you will:\n",
    "\n",
    "    ✓ Install the NVIDIA Container Toolkit on a server\n",
    "    \n",
    "    ✓ Run a container from NGC to train and recognize handwritten numbers\n",
    "\n",
    "### Prerequisites and Guidelines\n",
    "There are no prerequisites for this lab.\n",
    "\n",
    "### Notice\n",
    "Please follow the instructions below carefully to successfully complete the practice.\n",
    "If you encounter technical issues, please contact the NVIDIA Networking Academy team:\n",
    "nbu-academy-support@nvidia.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 1: Install the Container Toolkit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice objectives:\n",
    "\n",
    "In this exercise, you will install the NVIDIA Container Toolkit and check if the toolkit is installed and working properly by running a container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Install the Container Toolkit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Configure a production repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor --yes -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Update the packages list from the repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Install the NVIDIA Container Toolkit packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y nvidia-container-toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 - Configure a container runtime to enable support for NVIDIA GPUs via the NVIDIA Container Toolkit:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo nvidia-ctk runtime configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 - Restart the Docker service:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo systemctl restart docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Run the NVIDIA Container Toolkit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Run a container and check if the NVIDIA Container Toolkit operates as expected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does your output resemble the output as seen below?\n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA H100 NVL                Off |   00000000:B5:00.0 Off |                    0 |\n",
    "| N/A   51C    P0            124W /  400W |       1MiB /  95830MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "                                                                                         \n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                              |\n",
    "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
    "|        ID   ID                                                               Usage      |\n",
    "|=========================================================================================|\n",
    "|  No running processes found                                                             |\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 Questions:\n",
    "\n",
    "    ✓ What command is executed in the container?\n",
    "    \n",
    "    ✓ What is the driver version in your output?\n",
    "    \n",
    "    ✓ What is the CUDA version in your output?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 2: Docker Container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice objectives:\n",
    "\n",
    "In this exercise you will use a Docker container with an NVIDIA GPU to build a simple neural network to recognize a numeral digit from an image file.\n",
    "\n",
    "This exercise assumes that the previous exercise \"Install Container Toolkit\" was completed successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Install and Run PyTorch Docker Container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Run the following command to install a container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker pull nvcr.io/nvidia/pytorch:25.01-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Create a working folder for the project:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p docker_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Before running the container, you may want to use \"tmux\" or \"screen\" to run monitoring commands on the host. Alternatively, you can open a new SSH connection to the host.\n",
    "\n",
    "Run the container using the following command:\n",
    "\n",
    "```bash\n",
    "sudo docker run --gpus all --ipc=host -it --rm -v /home/ubuntu/notebooks/module8:/workspace nvcr.io/nvidia/pytorch:25.01-py3\n",
    "```\n",
    "\n",
    "**Note:** The directory `/home/ubuntu/notebooks/module8` on the host is mapped to the \"workspace\" directory in the container. This means that any file created in this directory on the host will appear in the container, and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Prepare Program Code Files for Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will train a model to recognize a numeral in a 24x24 pixel image.\n",
    "\n",
    "Navigate to the `~/notebooks/module8` directory, which contains the following files:\n",
    "\n",
    "    ✓ train.py – Program that trains the AI\n",
    "    \n",
    "    ✓ test.py – Program that uses the trained AI\n",
    "    \n",
    "    ✓ 2.png, 7.png and 8.png – Image files used to test the AI\n",
    "\n",
    "You can create a few 24x24 pixel images using software like MS Paint, writing a number in each image, or using files downloaded from GitHub. In our example, these are the images we used with significant magnification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/notebooks/module8/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Before you start training your model, run nvidia-smi and make sure to remember or write down the following information:\n",
    "\n",
    "    ✓ The GPU's memory usage before training begins\n",
    "    \n",
    "    ✓ The current GPU utilization\n",
    "    \n",
    "    ✓ The list of processes currently running on the GPU\n",
    "\n",
    "Retain these details for later reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Run the training using the command \"python train.py\". During the training, several epochs will run:\n",
    "\n",
    "**Note:** This command should be run from within the Docker container. You may need to execute the docker run command from Task 1.3 first.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```\n",
    "Train Epoch: 1 [0/60000 (0%)]   Loss: 2.303346             \n",
    "Train Epoch: 1 [6400/60000 (11%)]       Loss: 0.147270     \n",
    "Train Epoch: 1 [12800/60000 (21%)]      Loss: 0.200906     \n",
    "Train Epoch: 1 [19200/60000 (32%)]      Loss: 0.102331     \n",
    "Train Epoch: 1 [25600/60000 (43%)]      Loss: 0.226644     \n",
    "Train Epoch: 1 [32000/60000 (53%)]      Loss: 0.024340     \n",
    "Train Epoch: 1 [38400/60000 (64%)]      Loss: 0.034060     \n",
    "Train Epoch: 1 [44800/60000 (75%)]      Loss: 0.005021     \n",
    "Train Epoch: 1 [51200/60000 (85%)]      Loss: 0.002311     \n",
    "Train Epoch: 1 [57600/60000 (96%)]      Loss: 0.022788     \n",
    "                                                           \n",
    "Test set: Average loss: 0.0406, Accuracy: 9878/10000 (99%)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - While your model is training, run the nvidia-smi command again.\n",
    "\n",
    "Compare the current output to what you observed before starting the training. Take note of the following:\n",
    "\n",
    "    ✓ What was the GPU memory usage during training, compared to before you started?\n",
    "    \n",
    "    ✓ How did GPU utilization change during training?\n",
    "    \n",
    "    ✓ Which processes were running on the GPU during training?\n",
    "    \n",
    "    ✓ How much time did the model training take to complete?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import a simple helper script called `timer` that will compare GPU vs CPU performance at the end of this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Try the Trained Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - Use the test.py file to check if the trained model can recognize the numerals from the image files. \n",
    "\n",
    "By default, test.py will attempt to recognize the numeral in the file 7.png. To test a different file, provide its name as a parameter.\n",
    "\n",
    "**Note:** These commands should be run from within the Docker container.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```bash\n",
    "root@20a0e58202b3:/workspace/AIDC-Professional# python test.py \n",
    "Processing file: 7.png\n",
    "The predicted number is: 7\n",
    "Device : cuda\n",
    "Time : 3.02 seconds \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --gpus all --ipc=host --rm -v /home/ubuntu/notebooks/module8:/workspace -w /workspace nvcr.io/nvidia/pytorch:25.01-py3 python test.py | tee /tmp/gpu_7.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - Test with a different image file:\n",
    "\n",
    "```bash\n",
    "root@20a0e58202b3:/workspace/AIDC-Professional# python test.py 8.png\n",
    "Processing file: 8.png\n",
    "The predicted number is: 7\n",
    "Device : cuda\n",
    "Time : 3.02 seconds\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --gpus all --ipc=host --rm -v /home/ubuntu/notebooks/module8:/workspace -w /workspace nvcr.io/nvidia/pytorch:25.01-py3 python test.py 8.png | tee /tmp/gpu_8.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Compare the Load on GPU vs CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 - Use the command 'exit' to exit the running container.\n",
    "\n",
    "Restart the container, but this time use the command:\n",
    "\n",
    "```bash\n",
    "sudo docker run --ipc=host -it --rm -v /home/ubuntu/notebooks/module8:/workspace nvcr.io/nvidia/pytorch:25.01-py3\n",
    "```\n",
    "\n",
    "**Note:** The `--gpus all` flag has been removed. The container will now run CPU-only, without utilizing the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --ipc=host --rm -e CUDA_VISIBLE_DEVICES= -v /home/ubuntu/notebooks/module8:/workspace -w /workspace nvcr.io/nvidia/pytorch:25.01-py3 python test.py | tee /tmp/cpu_7.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 - The previously used files should still be in the container. Start training the model again.\n",
    "\n",
    "**Question:** How much time did the model training take compared to GPU training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --ipc=host --rm -e CUDA_VISIBLE_DEVICES= -v /home/ubuntu/notebooks/module8:/workspace -w /workspace nvcr.io/nvidia/pytorch:25.01-py3 python train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "## Summary: GPU vs CPU Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
